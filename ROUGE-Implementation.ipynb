{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Score-  \n",
    "Recall Oriented Understudy for Gisting Evaluation  \n",
    "Its a set of metrics used to estimate the quality of summary generated by a LLM/ other model.  \n",
    "It has various variants but mostly used are ROUGE-N and ROUGE-l  \n",
    "It generally uses overlaps of n-grams between generated string and reference sting or finds the Longest common subsequence of generated string and reference string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am implementing the ROUGE score from scratch in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the n-grams , n can be 1 for unigrams, 2 for bigrams ...\n",
    "\n",
    "def n_grams(tokens,n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)+1-n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing ROUGE-N first\n",
    "def rouge_n(generated,reference,n=1):\n",
    "    generated_n_grams=Counter(n_grams(generated,n))\n",
    "    reference_n_grams=Counter(n_grams(reference,n))\n",
    "    overlap_ngrams=generated_n_grams & reference_n_grams\n",
    "    overlap_count=sum(overlap_ngrams.values())\n",
    "    reference_count=sum(reference_n_grams.values())\n",
    "# calculate precision,recall and F1 score\n",
    "    precision=overlap_count/sum(generated_n_grams.values()) if generated_n_grams else 0\n",
    "    recall=overlap_count/reference_count if reference_count else 0\n",
    "    f1_score=(2*precision*recall)/(precision+recall) if (precision+recall)>0 else 0\n",
    "    return {\"precision\":precision,\"recall\":recall,\"f1_score\":f1_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,n=4,4\n",
    "dp=[[0]*(n+1) for _ in range(m+1)]\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing ROUGE-L where need to calculate LCS between 2 strings\n",
    "def rouge_l(generated,reference):\n",
    "    def lcs_length(x,y):\n",
    "        m,n=len(x),len(y)\n",
    "        dp=[[0]*(n+1) for _ in range(m+1)]\n",
    "        for i in range(1,m+1):\n",
    "            for j in range(1,n+1):\n",
    "                if x[i-1]==y[j-1]:\n",
    "                    dp[i][j]=dp[i-1][j-1]+1\n",
    "                else:\n",
    "                    dp[i][j]=max(dp[i-1][j],dp[i][j-1])\n",
    "        return dp[m][n]\n",
    "\n",
    "    lcs_len=lcs_length(generated,reference)\n",
    "    recall=lcs_len/len(reference) if reference else 0\n",
    "    precision=lcs_len/len(generated) if generated else 0\n",
    "    f1_score=(2*recall*precision)/(recall+precision) if (recall+precision)>0 else 0\n",
    "    return {\"precision\":precision,\"recall\":recall,\"f1_score\":f1_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing ROUGE score using some data available on internet.  \n",
    "Using 1-gram and ROUGE-L for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: {'precision': 0.47619047619047616, 'recall': 0.5617977528089888, 'f1_score': 0.5154639175257731}\n",
      "ROUGE-L: {'precision': 0.37142857142857144, 'recall': 0.43820224719101125, 'f1_score': 0.4020618556701031}\n"
     ]
    }
   ],
   "source": [
    "generated=\"The majestic mountains stood tall against the horizon, their peaks dusted with snow that glistened under the golden rays of the morning sun. The valley below was a patchwork of green fields and winding rivers, dotted with small villages that seemed to be untouched by time. Birds chirped merrily in the trees, and the air was filled with the scent of fresh pine and blooming flowers. It was a scene of perfect tranquility, where nature and humanity existed in harmony. As the day progressed, the sunlight shifted, casting long shadows across the landscape, and the mountains took on a deep purple hue as dusk approached.\".split()\n",
    "reference=\"The towering mountains dominated the landscape, their snow-capped peaks glowing in the early morning light. Below, the valley stretched out in a mosaic of fields and rivers, with quaint villages nestled among the greenery. The chirping of birds filled the air, blending with the scent of pine trees and blooming flowers. This serene scene was a testament to the peaceful coexistence of nature and mankind. As the day wore on, the light changed, creating dramatic shadows on the mountainside, which turned a rich purple as the evening drew near\".split()\n",
    "\n",
    "rouge_1_result=rouge_n(generated,reference,n=1)\n",
    "print(\"ROUGE-1:\",rouge_1_result)\n",
    "\n",
    "rouge_l_result=rouge_l(generated,reference)\n",
    "print(\"ROUGE-L:\",rouge_l_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If want to use inbuilt library- use rouge-score for the same.  \n",
    "Download and install it using pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE1 - Precision: 0.622222, Recall: 0.533333, F1-Score: 0.574359\n",
      "ROUGE2 - Precision: 0.168539, Recall: 0.144231, F1-Score: 0.155440\n",
      "ROUGEL - Precision: 0.455556, Recall: 0.390476, F1-Score: 0.420513\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Define the two paragraphs\n",
    "paragraph1=(\"The majestic mountains stood tall against the horizon, \"\n",
    "              \"their peaks dusted with snow that glistened under the golden rays of the morning sun. \"\n",
    "              \"The valley below was a patchwork of green fields and winding rivers, dotted with small villages \"\n",
    "              \"that seemed to be untouched by time. Birds chirped merrily in the trees, and the air was filled \"\n",
    "              \"with the scent of fresh pine and blooming flowers. It was a scene of perfect tranquility, \"\n",
    "              \"where nature and humanity existed in harmony. As the day progressed, the sunlight shifted, \"\n",
    "              \"casting long shadows across the landscape, and the mountains took on a deep purple hue as dusk approached.\")\n",
    "\n",
    "paragraph2=(\"The towering mountains dominated the landscape, their snow-capped peaks glowing in the early morning light. \"\n",
    "              \"Below, the valley stretched out in a mosaic of fields and rivers, with quaint villages nestled among the greenery. \"\n",
    "              \"The chirping of birds filled the air, blending with the scent of pine trees and blooming flowers. \"\n",
    "              \"This serene scene was a testament to the peaceful coexistence of nature and mankind. \"\n",
    "              \"As the day wore on, the light changed, creating dramatic shadows on the mountainside, \"\n",
    "              \"which turned a rich purple as the evening drew near.\")\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer=rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'],use_stemmer=True)\n",
    "scores=scorer.score(paragraph1,paragraph2)\n",
    "\n",
    "# Print the results\n",
    "for rouge_type,score in scores.items():\n",
    "    print(f\"{rouge_type.upper()} - Precision: {score.precision:.6f}, Recall: {score.recall:.6f}, F1-Score: {score.fmeasure:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
